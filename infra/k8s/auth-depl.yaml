# selects the "bucket" of kubernetes objects we want to access
apiVersion: apps/v1
# from that bucket, which kind of objects do we want to create?
kind: Deployment
metadata:
  name: auth-depl
spec:
  # How many pods do you want this deployment to run? This is what replicas specifies
  replicas: 1
  # Tells the Deployment which are the pods it needs to manage
  selector:
    matchLabels:
      # This is a label. This is how the Deployment is going to find the pods it needs to manage.
      # The whole label/selector system is very similar to HTML tags
      # The key could be anything really. `app` doesn't have a specific meaning.
      app: auth
  # Template for the pods - essentially the content of a pod config file; defines how every pod that is created and managed
  # by this deployment is going to look like and should behave:
  template:
    metadata:
      # This ties in with `matchLabels` selector spec above - i.e. the Deployment is going to create pods with the label app: auth
      # (and these are the ones that the selector above will find)
      labels:
        app: auth
    # Spec defines how every pod that is created and managed by this deployment is going to behave:
    spec:
      containers:
        # dash means that the following is an array entity
        - name: auth
          # If we don't add the version number at the end, it means implicitly `busdav/posts:latest`.
          # That gives kubernetes ("k8s") the instruction to go to docker hub and look for latest version.
          # If however we put a specific version, k8s will use that one if it finds it on the machine it is running on.
          image: us.gcr.io/fleet-parser-310206/auth
          env:
            # This will be name of the environment variable as it shows up inside of the auth container:
            - name: MONGO_URI
              value: "mongodb://auth-mongo-srv:27017/auth"
            - name: JWT_KEY
              valueFrom:
                secretKeyRef:
                  name: jwt-secret
                  key: JWT_KEY

# Whenever you create a deployment, you usually also want to create a cluster IP Service (which is a Kubernetes service),
# in order to give you access to a pod *from within your cluster*.
# A cluster IP service is going to allow communication to that cluster IP service from anything else running (only) inside of our cluster.
# To colocate  the cluster IP config in the same file as the deployment config file, you can do that via the
# three dashes separator. (You could also create a separate file for it, but often there will be a 1:1 ratio between our
# deployments and our (cluster IP) services, so it kind of makes sense to put everything into one file.)
---
apiVersion: v1
kind: Service
metadata:
  name: auth-srv
spec:
  # The selector is how the service is going to find the pods that the service is supposed to govern requests to.
  selector:
    app: auth
  # By default, K8s will create a type of ClusterIP, so next line is optional
  type: ClusterIP
  # Now we specify the list of ports that we want to allow access to:
  ports:
    # Name of the port is not super important - is more for logging and reporting purposes.
    - name: auth
      protocol: TCP
      # Port of the k8s service that we're configuring:
      port: 3000
      # Port of the k8s pod (with our container) where that service should be routing traffic to:
      # Port should match the port that the express app of this microservice, running inside the container, is listening on:
      targetPort: 3000
